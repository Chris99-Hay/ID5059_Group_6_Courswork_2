# ID5059 Coursework 2 Credit Card Fradulent Data


################################################################################
# Load Libraries and Data as needed
# We load full data set and the split test and training datasets
################################################################################
setwd("D:/")
train <- read.csv("train.csv")
test.data <- read.csv("test.csv")
df <- read.csv("creditcard.csv")

str(df)
head(df)

library(dplyr) # for data manipulation
library(stringr) # for data manipulation
library(caret) # for sampling
library(caTools) # for train/test split
library(ggplot2) # for data visualization
library(corrplot) # for correlations
install.packages("Rtsne")
library(Rtsne) # for tsne plotting
install.packages("DMwR")
install.packages("performanceEstimation")
install.packages(c("zoo","xts","quantmod")) ## and perhaps mode

library(performanceEstimation) # for smote implementation
install.packages("ROSE")
library(ROSE)# for ROSE sampling
library(rpart)# for decision tree model
install.packages("Rborist")
library(Rborist)# for random forest model
install.packages("xgboost")
library(xgboost) # for xgboost model

# function to set plot height and width
fig <- function(width, heigth){
  options(repr.plot.width = width, repr.plot.height = heigth)
}

# Check for NAs 
colSums(is.na(df)) # no missing data it seems

# Check class imbalance
table(df$Class)
#    0      1 
# 284315    492

# class imbalance in percentage
prop.table(table(df$Class))
#         0           1 
# 0.998272514 0.001727486 





################################################################################
#  Data Visualization
################################################################################
# Visualize the imbalance in a bar chat format
# clearly there is significant imbalance as no fraud is 100% and fraud is 0%
fig(12, 8)
common_theme <- theme(plot.title = element_text(hjust = 0.5, face = "bold"))

ggplot(data = df, aes(x = factor(Class), 
                      y = prop.table(stat(count)), fill = factor(Class),
                      label = scales::percent(prop.table(stat(count))))) +
  geom_bar(position = "dodge") + 
  geom_text(stat = 'count',
            position = position_dodge(.9), 
            vjust = -0.5, 
            size = 3) + 
  scale_x_discrete(labels = c("no fraud", "fraud"))+
  scale_y_continuous(labels = scales::percent)+
  labs(x = 'Class', y = 'Percentage') +
  ggtitle("Distribution of class labels") +
  common_theme

###
# Distribution of Time by class
###
fig(14, 8)
df %>%
  ggplot(aes(x = Time, fill = factor(Class))) + geom_histogram(bins = 100)+
  labs(x = 'Time in seconds since first transaction', y = 'No. of transactions') +
  ggtitle('Distribution of time of transaction by class') +
  facet_grid(Class ~ ., scales = 'free_y') + common_theme

# The ‘Time’ feature looks pretty similar across both types of transactions. 
# One could argue that fraudulent transactions are more uniformly distributed,
# while normal transactions have a cyclical distribution.

###
# Distribution of Transaction amount by Class
###
fig(14, 8)
ggplot(df, aes(x = factor(Class), y = Amount)) + geom_boxplot() + 
  labs(x = 'Class', y = 'Amount') +
  ggtitle("Distribution of transaction amount by class") + common_theme

# More variability in the transaction values for non-fraudulent transactions?
# More outliers

###
# Correlation Plot Matrix (could try covariance matrix as well...)
###
fig(14, 8)
correlations <- cor(df[,-1],method="pearson")
corrplot(correlations, number.cex = .9, method = "number", type = "full", tl.cex=0.8,tl.col = "black")
# No variables stand out as signifcantly correlated with Amount or Class
# This may be because before publishing, most of the features were presented 
# to a Principal Component Analysis (PCA) algorithm. 
# The features V1 to V28 are most probably the Principal Components resulted after 
# propagating the real features through PCA.
# We do not know if the numbering of the features reflects the importance of the Principal Components.

###
# Visualization of transactions using t-SNE
###

# Try visualizing the data using t-Distributed Stochastic Neighbour Embedding, 
# a technique to reduce dimensionality.
# To train the model, perplexity was set to 20.
# The visualisation should give us a hint as to whether there exist any 
# “discoverable” patterns in the data which the model could learn.
# If there is no obvious structure in the data, it is more likely that the 
# model will perform poorly.

fig(16, 10)

# Use 10% of data to compute t-SNE
tsne_subset <- 1:as.integer(0.1*nrow(df))
tsne <- Rtsne(df[tsne_subset,-c(1, 31)], perplexity = 20, theta = 0.5, pca = F,
              verbose = F, max_iter = 500, check_duplicates = F)

classes <- as.factor(df$Class[tsne_subset])
tsne_mat <- as.data.frame(tsne$Y)
ggplot(tsne_mat, aes(x = V1, y = V2)) + geom_point(aes(color = classes)) + 
  theme_minimal() + common_theme + ggtitle("t-SNE visualisation of transactions") + 
  scale_color_manual(values = c("#E69F00", "#56B4E9"))
# There appears to be a separation between the two classes as most fraudulent 
# transactions seem to lie near the edge of the blob of data.

################################################################################
#  Data Cleaning/Preparation for Modelling
# Time’ feature has little or no significance in correctly classifying a fraud 
# transaction and hence eliminate this column from further analysis.
################################################################################
#Remove 'Time' variable
df <- df[,-1]
#Change 'Class' variable to factor
df$Class <- as.factor(df$Class)
levels(df$Class) <- c("Not_Fraud", "Fraud")
#Scale numeric variables
df[,-30] <- scale(df[,-30])
head(df)

### 
# We can split the full dataset ourselves
###
set.seed(123)
split <- sample.split(df$Class, SplitRatio = 0.7)
train <-  subset(df, split == TRUE)
test <- subset(df, split == FALSE)
# We can use an oversampling method with the minority class of fraudulent data to
# rebalance the data.  Using random oversampling can prevent information loss, but
# we lead to overfitting later on

### We can also use Synthetic Minority Oversampling Technique or SMOTE
# Powerful and commonly used

# downsampling
set.seed(9560)
down_train <- downSample(x = train[, -ncol(train)],
                         y = train$Class)
table(down_train$Class)

# rose
set.seed(9560)
rose_train <- ROSE(Class ~ ., data  = train)$data 

table(rose_train$Class)

# Oversampling
set.seed(9560)
up_train <- upSample(x = train[, -ncol(train)],
                     y = train$Class)
table(up_train$Class)

# smote (Cannot use DMwR package...no longer exists)
set.seed(9560)
smote_train <- smote(Class ~ ., data  = train)

table(smote_train$Class)

################################################################################
# Run Models
################################################################################


###
# CART Model
###

# CART Model Performance on imbalanced data
set.seed(5627)

orig_fit <- rpart(Class ~ ., data = train)

#Evaluate model performance on test set
pred_orig <- predict(orig_fit, newdata = test, method = "class")

roc.curve(test$Class, pred_orig[,2], plotit = TRUE)

# CART Downsample
down_fit <- rpart(Class ~ ., data = down_train)

# CART for Oversample
up_fit <- rpart(Class ~ ., data = up_train)

# CART for SMOTE
smote_fit <- rpart(Class ~ ., data = smote_train)

# CART ROSE 
rose_fit <- rpart(Class ~ ., data = rose_train)


# AUC on down-sampled data
pred_down <- predict(down_fit, newdata = test)

print('Fitting model to downsampled data')
roc.curve(test$Class, pred_down[,2], plotit = FALSE) # 0.942

# AUC on up-sampled data
pred_up <- predict(up_fit, newdata = test)

print('Fitting model to upsampled data')
roc.curve(test$Class, pred_up[,2], plotit = FALSE) # 0.943

# AUC on SMOTE data
pred_smote <- predict(smote_fit, newdata = test)

print('Fitting model to smote data')
roc.curve(test$Class, pred_smote[,2], plotit = FALSE) # 0.934

# # AUC on ROSE data
pred_rose <- predict(rose_fit, newdata = test)

print('Fitting model to rose data')
roc.curve(test$Class, pred_rose[,2], plotit = FALSE) # 0.942

###
## Upsampled model seemed to perform best so we will cotninue with that data set
###


###
# Logistic Regression
###
glm_fit <- glm(Class ~ ., data = up_train, family = 'binomial')

pred_glm <- predict(glm_fit, newdata = test, type = 'response')

roc.curve(test$Class, pred_glm, plotit = TRUE)# 0.971


###
# Random Forest
###
x = up_train[, -30]
y = up_train[,30]

rf_fit <- Rborist(x, y, ntree = 1000, minNode = 20, maxLeaf = 13)


rf_pred <- predict(rf_fit, test[,-30], ctgCensus = "prob")
prob <- rf_pred$prob

roc.curve(test$Class, prob[,2], plotit = TRUE) # 0.976


###
# XGBoost
###
# Convert class labels from factor to numeric

labels <- up_train$Class

y <- recode(labels, 'Not_Fraud' = 0, "Fraud" = 1)

set.seed(42)
xgb <- xgboost(data = data.matrix(up_train[,-30]), 
               label = y,
               eta = 0.1,
               gamma = 0.1,
               max_depth = 10, 
               nrounds = 300, 
               objective = "binary:logistic",
               colsample_bytree = 0.6,
               verbose = 0,
               nthread = 7,
)

xgb_pred <- predict(xgb, data.matrix(test[,-30]))

roc.curve(test$Class, xgb_pred, plotit = TRUE)# 0.977

# Feature Importance
names <- dimnames(data.matrix(up_train[,-30]))[[2]]
# Compute feature importance matrix
importance_matrix <- xgb.importance(names, model = xgb)
# Nice graph
xgb.plot.importance(importance_matrix[1:10,])

# With an auc score of 0.977 the XGBOOST model has performed the best though 
# both the random forest and logistic regression
